{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajgquional/Coursera_Deploying-ML-Models-in-Prod/blob/main/C4_W2_Lab_4_Apache_Beam_and_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DwycGzXvRVx"
      },
      "source": [
        "# Ungraded Lab (Optional): ETL Pipelines and Batch Predictions with Apache Beam and Tensorflow\n",
        "\n",
        "In this lab, you will create, train, evaluate, and make predictions on a model using [Apache Beam](https://beam.apache.org/) and [TensorFlow](https://www.tensorflow.org/). In particular, you will train a model to predict the molecular energy based on the number of carbon, hydrogen, oxygen, and nitrogen atoms.\n",
        "\n",
        "This lab is marked as optional because you will not be interacting with Beam-based systems directly in future exercises. Other courses of this specialization also use tools that abstract this layer. Nonetheless, it would be good to be familiar with it since it is used under the hood by TFX which is the main ML pipelines framework that you will use in other labs. Seeing how these systems work will let you explore other codebases that use this tool more freely and even make contributions or bug fixes as you see fit. If you don't know the basics of Beam yet, we encourage you to look at the [Minimal Word Count example here](https://beam.apache.org/get-started/wordcount-example/) for a quick start and use the [Beam Programming Guide](https://beam.apache.org/documentation/programming-guide) to look up concepts if needed.\n",
        "\n",
        "The entire pipeline can be divided into four phases:\n",
        " 1. Data extraction\n",
        " 2. Preprocessing the data\n",
        " 3. Training the model\n",
        " 4. Doing predictions\n",
        "\n",
        "You will focus particularly on Phase 2 (Preprocessing) and a bit of Phase 4 (Predictions) because these use Beam in its implementation.\n",
        "\n",
        "Let's begin!\n",
        "\n",
        "*Note: This tutorial uses code, images, and discussion from [this  article](https://cloud.google.com/dataflow/examples/molecules-walkthrough). We highlighted a few key parts and updated some of the code to use more recent versions. Also, we focused on making the lab running locally. The original article linked above contain instructions on running it in GCP. Just take note that it will have associated costs depending on the resources you use.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1lasCIx1upi"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "You will first setup the environment and download the scripts that you will use in the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbyQLdMoZAbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c94681-c1cc-49be-86d2-08f1ea8ca00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib python3-setuptools python3-wheel python3.7-lib2to3\n",
            "  python3.7-minimal\n",
            "Suggested packages:\n",
            "  python-setuptools-doc python3.7-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib python3-pip python3-setuptools python3-wheel python3.7\n",
            "  python3.7-distutils python3.7-lib2to3 python3.7-minimal\n",
            "0 upgraded, 9 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 6,661 kB of archives.\n",
            "After this operation, 27.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.2 [340 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1,306 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-minimal amd64 3.7.17-1+jammy1 [608 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-minimal amd64 3.7.17-1+jammy1 [1,837 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-stdlib amd64 3.7.17-1+jammy1 [1,864 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7 amd64 3.7.17-1+jammy1 [362 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-lib2to3 all 3.7.17-1+jammy1 [124 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-distutils all 3.7.17-1+jammy1 [189 kB]\n",
            "Fetched 6,661 kB in 9s (710 kB/s)\n",
            "Selecting previously unselected package libpython3.7-minimal:amd64.\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-minimal.\n",
            "Preparing to unpack .../1-python3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.7-minimal (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.7-stdlib:amd64.\n",
            "Preparing to unpack .../2-libpython3.7-stdlib_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../3-python3-setuptools_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../4-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../5-python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Selecting previously unselected package python3.7.\n",
            "Preparing to unpack .../6-python3.7_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.7 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-lib2to3.\n",
            "Preparing to unpack .../7-python3.7-lib2to3_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-distutils.\n",
            "Preparing to unpack .../8-python3.7-distutils_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3.7-minimal (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Setting up libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7 (3.7.17-1+jammy1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in auto mode\n"
          ]
        }
      ],
      "source": [
        "# NOTE: Some of the packages used in this lab are not compatible with the\n",
        "# default Python version in Colab (Python3.8 as of January 2023).\n",
        "# The commands below will setup the environment to use Python3.7 instead.\n",
        "# Please *DO NOT* restart the runtime after running these.\n",
        "\n",
        "# Install packages needed to downgrade to Python3.7\n",
        "!apt-get install python3.7 python3.7-distutils python3-pip\n",
        "\n",
        "# Configure the Colab environment to use Python3.7 by default\n",
        "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAlhjfVPUOpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31264e2b-0295-46e6-d626-71a08a38f94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-08 05:39:06--  https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/data/molecules.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/data/molecules.tar.gz [following]\n",
            "--2025-04-08 05:39:06--  https://raw.githubusercontent.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/data/molecules.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10603 (10K) [application/octet-stream]\n",
            "Saving to: ‘molecules.tar.gz’\n",
            "\n",
            "molecules.tar.gz    100%[===================>]  10.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-08 05:39:07 (64.7 MB/s) - ‘molecules.tar.gz’ saved [10603/10603]\n",
            "\n",
            "data-extractor.py\n",
            "predict.py\n",
            "preprocess.py\n",
            "pubchem/\n",
            "pubchem/__init__.py\n",
            "pubchem/sdf.py\n",
            "pubchem/pipeline.py\n",
            "trainer/\n",
            "trainer/task.py\n",
            "trainer/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Download the scripts\n",
        "!wget https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/data/molecules.tar.gz\n",
        "\n",
        "# Unzip the archive\n",
        "!tar -xvzf molecules.tar.gz --one-top-level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDn4G_ksYVz4"
      },
      "source": [
        "The `molecules` directory you downloaded mainly contain 4 scripts that encapsulate all phases of the workflow you will execute in this lab. It is summarized by the figure below:\n",
        "\n",
        "<img src='https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/images/overview.png' alt='https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/images/overview.png'>\n",
        "\n",
        "It also includes the `pubchem` subdirectory which contains common modules (i.e. `pipeline.py` and `sdf.py`) shared by the preprocessing and predicition phases. If you look at `preprocess.py` and `predict.py`, you can see the line `import as pubchem` at the top."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK6XzQ9ExadV"
      },
      "source": [
        "You will then install some packages needed in this notebook. You will use the Apache Beam version bundled with Tensorflow Transform. You can safely ignore the incompatibility error shown after installing `dill`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kbV5ABcreE7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "690373df-46be-4e08-d77a-a365a9797f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-transform==1.12\n",
            "  Downloading tensorflow_transform-1.12.0-py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.8/439.8 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-metadata<1.13.0,>=1.12.0\n",
            "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py<2.0.0,>=0.9\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.41\n",
            "  Downloading apache_beam-2.48.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tfx-bsl<1.13.0,>=1.12.0\n",
            "  Downloading tfx_bsl-1.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (21.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2,>=1.16\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow<7,>=6\n",
            "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydot<2,>=1.2\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<5.0.0,>=3.8.0\n",
            "  Downloading pymongo-4.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (655 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m655.7/655.7 KB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod<2.0,>=1.7\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting grpcio!=1.48.0,<2,>=1.33.1\n",
            "  Downloading grpcio-1.62.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.21.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.7.0\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting cloudpickle~=2.2.1\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil<3,>=2.8.0\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/lib/python3/dist-packages (from apache-beam[gcp]<3,>=2.41->tensorflow-transform==1.12) (0.20.2)\n",
            "Collecting objsize<0.7.0,>=0.6.1\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.9.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2018.3\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2020.6.8\n",
            "  Downloading regex-2024.4.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.6/761.6 KB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-httplib2<0.2.0,>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting google-cloud-bigquery<4,>=2.0.0\n",
            "  Downloading google_cloud_bigquery-3.30.0-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.9/247.9 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-datastore<3,>=2.0.0\n",
            "  Downloading google_cloud_datastore-2.20.2-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.29.0-py2.py3-none-any.whl (317 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-cloud-bigquery-storage<3,>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.30.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.29.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.9/213.9 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.18.0\n",
            "  Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.8/210.8 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<6,>=3.1.0\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Collecting google-cloud-spanner<4,>=3.0.0\n",
            "  Downloading google_cloud_spanner-3.53.0-py2.py3-none-any.whl (483 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.1/483.1 KB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-videointelligence<3,>=2.0\n",
            "  Downloading google_cloud_videointelligence-2.16.1-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.5/274.5 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n",
            "  Downloading google_cloud_pubsublite-1.7.0-py2.py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.9/273.9 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigtable<2.18.0,>=2.0.0\n",
            "  Downloading google_cloud_bigtable-2.17.0-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.6/288.6 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-language<3,>=2.0\n",
            "  Downloading google_cloud_language-2.17.1-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2\n",
            "  Downloading google_cloud_vision-3.10.1-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.1/526.1 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<0.11.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.10.17-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.3/211.3 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-core<3,>=2.0.0\n",
            "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/lib/python3/dist-packages (from pydot<2,>=1.2->tensorflow-transform==1.12) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-transform==1.12) (59.6.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging\n",
            "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-transform==1.12) (1.16.0)\n",
            "INFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pydot<2,>=1.2\n",
            "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyarrow<7,>=6\n",
            "  Downloading pyarrow-6.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting numpy<2,>=1.16\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of apache-beam[gcp] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting apache-beam[gcp]<3,>=2.41\n",
            "  Downloading apache_beam-2.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigtable<3,>=2.0.0\n",
            "  Downloading google_cloud_bigtable-2.30.0-py3-none-any.whl (484 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.1/484.1 KB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<5,>=3.1.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting apache-beam[gcp]<3,>=2.41\n",
            "  Downloading apache_beam-2.46.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (506 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.0/506.0 KB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-datastore<2,>=1.8.0\n",
            "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.2/134.2 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigquery-storage<2.17,>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.16.2-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.4/185.4 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.3-py2.py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libclang>=13.0.0\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\n",
            "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.2/293.2 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.11.1-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pandas<2,>=1.0\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client<2,>=1.7.11\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-transform==1.12) (0.37.1)\n",
            "Collecting google-api-core<3dev,>=1.21.0\n",
            "  Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting oauth2client>=1.4.12\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=2.0.0\n",
            "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: importlib-metadata>1.0.0 in /usr/lib/python3/dist-packages (from google-cloud-core<3,>=2.0.0->apache-beam[gcp]<3,>=2.41->tensorflow-transform==1.12) (4.6.4)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.28.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.28.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.4/210.4 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.27.0-py2.py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.26.0-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.25.1-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.6/197.6 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.25.0-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.9/197.9 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.24.0-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.23.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.22.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.21.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.20.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.19.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.18.1-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.4/180.4 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.18.0-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api<=1.22.0\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-sdk<=1.22.0\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.28.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.5/301.5 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.27.3-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.27.2-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.7/289.7 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.27.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.27.0-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.26.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.26.0-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.25.2-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.25.1-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 KB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.25.0-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.23.1-py2.py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.2/277.2 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.23.0-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.22.0-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.2/276.2 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.21.5-py2.py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.21.4-py2.py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-status>=1.33.2\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<7.0.0,>=6.0.1\n",
            "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
            "Collecting google-cloud-spanner<4,>=3.0.0\n",
            "  Downloading google_cloud_spanner-3.52.0-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.9/449.9 KB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_spanner-3.51.0-py2.py3-none-any.whl (432 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.6/432.6 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_spanner-3.50.1-py2.py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.5/416.5 KB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_spanner-3.50.0-py2.py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.4/416.4 KB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_spanner-3.49.1-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.7/402.7 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_spanner-3.48.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.4/397.4 KB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_spanner-3.47.0-py2.py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.6/384.6 KB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlparse>=0.4.4\n",
            "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpc-interceptor>=0.15.4\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Collecting google-cloud-spanner<4,>=3.0.0\n",
            "  Downloading google_cloud_spanner-3.46.0-py2.py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.2/381.2 KB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2\n",
            "  Downloading google_cloud_vision-3.10.0-py2.py3-none-any.whl (523 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.4/523.4 KB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.6/514.6 KB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.8.1-py2.py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.9/486.9 KB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.8.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.5/488.5 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 KB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.7.3-py2.py3-none-any.whl (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.4/466.4 KB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.7.2-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.6/459.6 KB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\n",
            "  Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.2/293.2 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.69.0-py2.py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.7/169.7 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.7/221.7 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.64.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.2/229.2 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.6/138.6 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-transform==1.12) (3.3.6)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 KB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Collecting grpcio-status>=1.33.2\n",
            "  Downloading grpcio_status-1.62.2-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.62.1-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.62.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.61.3-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.60.2-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.60.1-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.60.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.59.5-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.59.3-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.59.2-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.59.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.58.3-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.58.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.57.0-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.56.2-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.56.0-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.55.3-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.54.3-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.54.2-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.53.2-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.53.1-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.53.0-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
            "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
            "Collecting pyasn1>=0.1.7\n",
            "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-transform==1.12) (3.2.0)\n",
            "Building wheels for collected packages: crcmod, dill, google-apitools, hdfs, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18850 sha256=f1a37228776e2e92055fe819f5d5eebb057dead30abab448dbfd0cf77a8f5732\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/9a/e9/49e627353476cec8484343c4ab656f1e0d783ee77b9dde2d1f\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=4e5f7af3e8ec236fd014574718a18527e958eb3599cd6a33bfacf0cc8fe027d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=f4245fe5befaecee3138b64b8d4b29164869280b544d1bdb8ba952ef8e4c2758\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34347 sha256=db79ba6d1651a12321e0e29b57290422de4b0ce2349cefa9259451c655a3aaf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/10/df/572595c13daae190b86e10aca0836fd31bc77cf9c4389252c1\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=9d95b2e9ee59eb6e9d5f07a05851d2188c3c30b57b9410011de086c54fa701f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built crcmod dill google-apitools hdfs docopt\n",
            "Installing collected packages: tensorboard-plugin-wit, pytz, libclang, flatbuffers, docopt, crcmod, zstandard, wrapt, urllib3, uritemplate, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sqlparse, regex, python-dateutil, pymongo, pydot, pyasn1, protobuf, packaging, overrides, orjson, objsize, numpy, MarkupSafe, keras, idna, grpcio, google-pasta, google-crc32c, gast, fasteners, fastavro, dill, cloudpickle, charset-normalizer, certifi, cachetools, astunparse, absl-py, werkzeug, rsa, requests, pyasn1-modules, pyarrow, proto-plus, pandas, opt-einsum, h5py, grpc-interceptor, googleapis-common-protos, google-resumable-media, tensorflow-metadata, requests-oauthlib, oauth2client, hdfs, grpcio-status, google-auth, grpc-google-iam-v1, google-auth-oauthlib, google-auth-httplib2, google-apitools, google-api-core, apache-beam, tensorboard, google-cloud-core, google-api-python-client, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow-transform\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Not uninstalling markupsafe at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n",
            "Successfully installed MarkupSafe-2.1.5 absl-py-1.4.0 apache-beam-2.46.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.8.0 fasteners-0.19 flatbuffers-25.2.10 gast-0.4.0 google-api-core-2.24.2 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-2.38.0 google-auth-httplib2-0.1.1 google-auth-oauthlib-0.4.6 google-cloud-bigquery-3.30.0 google-cloud-bigquery-storage-2.16.2 google-cloud-bigtable-1.7.3 google-cloud-core-2.4.3 google-cloud-datastore-1.15.5 google-cloud-dlp-3.18.0 google-cloud-language-1.3.2 google-cloud-pubsub-2.21.4 google-cloud-pubsublite-1.7.0 google-cloud-recommendations-ai-0.7.1 google-cloud-spanner-3.46.0 google-cloud-videointelligence-1.16.3 google-cloud-vision-3.7.2 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.7.2 googleapis-common-protos-1.63.1 grpc-google-iam-v1-0.12.7 grpc-interceptor-0.15.4 grpcio-1.62.3 grpcio-status-1.48.2 h5py-3.8.0 hdfs-2.7.3 idna-3.10 keras-2.11.0 libclang-18.1.1 numpy-1.21.6 oauth2client-4.1.3 objsize-0.6.1 opt-einsum-3.3.0 orjson-3.9.7 overrides-6.5.0 packaging-24.0 pandas-1.3.5 proto-plus-1.26.1 protobuf-3.19.6 pyarrow-6.0.1 pyasn1-0.5.1 pyasn1-modules-0.3.0 pydot-1.4.2 pymongo-3.13.0 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2024.4.16 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9 sqlparse-0.4.4 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-metadata-1.12.0 tensorflow-serving-api-2.11.0 tensorflow-transform-1.12.0 termcolor-2.3.0 tfx-bsl-1.12.0 typing-extensions-4.7.1 uritemplate-3.0.1 urllib3-2.0.7 werkzeug-2.2.3 wrapt-1.16.0 zstandard-0.21.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "google"
                ]
              },
              "id": "dfca2c9becde4e60aa7f365c9fccfc02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill==0.3.3\n",
            "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# IMPORTANT NOTE: Please DO NOT restart the runtime after running the commands\n",
        "# below. Doing so might cause issues because the Python runtime was switched.\n",
        "\n",
        "# Install packages\n",
        "!pip install tensorflow-transform==1.12\n",
        "!pip install dill==0.3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT5UJpLn2KgV"
      },
      "source": [
        "_Note: Please **DO NOT** restart the runtime after installing the packages. Doing so might cause issues because the Python version was switched. If you restarted it by mistake, kindly go to `Runtime > Disconnect and delete runtime`, and then re-run the previous cells._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNr3GXCp5pJo"
      },
      "source": [
        "Next, you'll define the working directory to contain all the results you will generate in this exercise. After each phase, you can open the Colab file explorer on the left and look under the `results` directory to see the new files and directories generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No28Orjtq6qu"
      },
      "outputs": [],
      "source": [
        "# Define working directory\n",
        "WORK_DIR = \"results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgiD7LTlG_4"
      },
      "source": [
        "With that, you are now ready to execute the pipeline. As shown in the figure earlier, the logic is already implemented in the four main scripts. You will run them one by one and the next sections will discuss relevant detail and the outputs generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSEoPhqf59bf"
      },
      "source": [
        "## Phase 1: Data extraction\n",
        "\n",
        "The first step is to extract the input data. The dataset  is stored as [`SDF`](https://en.wikipedia.org/wiki/Chemical_table_file#SDF) files and is extracted from the [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/) [(FTP source)](ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound_3D/01_conf_per_cmpd/SDF). Chapter 6 of [this document](http://c4.cabrillo.edu/404/ctfile.pdf) shows a more detailed description of the SDF file format.\n",
        "\n",
        "The `data-extractor.py` file extracts and decompresses the specified SDF files. In later steps, the example preprocesses these files and uses the data to train and evaluate the machine learning model. The file extracts the SDF files from the public source and stores them in a subdirectory inside the specified working directory.\n",
        "\n",
        "As you can see [here](https://ftp.ncbi.nlm.nih.gov/pubchem/Compound_3D/01_conf_per_cmpd/SDF/), the complete set of files is huge and can easily exceed storage limits in Colab. For this exercise, you will just download one file. You can use the script as shown in the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD5nYt6enHsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934fd05f-bc94-4b31-8c05-464b43cbbae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 05:44:30.413825: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-08 05:44:30.619355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:44:30.619393: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-04-08 05:44:31.968994: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:44:31.969362: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:44:31.969401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "usage: data-extractor.py [-h] --work-dir WORK_DIR [--data-sources DATA_SOURCES [DATA_SOURCES ...]]\n",
            "                         [--filter-regex FILTER_REGEX] --max-data-files MAX_DATA_FILES\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --work-dir WORK_DIR   Directory for staging and working files. This can be a Google Cloud\n",
            "                        Storage path. (default: None)\n",
            "  --data-sources DATA_SOURCES [DATA_SOURCES ...]\n",
            "                        Data source location where SDF file(s) are stored. Paths can be local,\n",
            "                        ftp://<path>, or gcs://<path>. Examples: ftp://hostname/path\n",
            "                        ftp://username:password@hostname/path (default: ['ftp://anonymous:guest@ft\n",
            "                        p.ncbi.nlm.nih.gov/pubchem/Compound_3D/01_conf_per_cmpd/SDF'])\n",
            "  --filter-regex FILTER_REGEX\n",
            "                        Regular expression to filter which files to use. The regular expression\n",
            "                        will be searched on the full absolute path. Every match will be kept.\n",
            "                        (default: \\.sdf)\n",
            "  --max-data-files MAX_DATA_FILES\n",
            "                        Maximum number of data files for every file pattern expansion. Set to -1\n",
            "                        to use all files. (default: None)\n"
          ]
        }
      ],
      "source": [
        "# Print the help documentation. You can ignore references to GCP because you will be running everything in Colab.\n",
        "!python ./molecules/data-extractor.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pti5F3n3G4L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b35690-f4a3-4e02-ebb7-2a252126498d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 05:44:47.795968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-08 05:44:48.051023: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:44:48.051110: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-04-08 05:44:49.388797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:44:49.388966: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:44:49.388992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Found 6814 files, using 1\n",
            "Extracting data files...\n",
            "Extracted results/data/00000001_00025000.sdf\n"
          ]
        }
      ],
      "source": [
        "# Run the data extractor\n",
        "!python ./molecules/data-extractor.py --max-data-files 1 --work-dir={WORK_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8InzfUTpi5p"
      },
      "source": [
        "You should now have a new folder in your work directory called `data`. This will contain the SDF file you downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WPtb0-RWbGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7720e06a-4ccc-41df-a567-7f1937008272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\n"
          ]
        }
      ],
      "source": [
        "# List working directory\n",
        "!ls {WORK_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ZguM0oqQjF"
      },
      "source": [
        "In the SDF Documentation linked earlier, it shows that one record is terminated by `$$$$`. You can use the command below to print the first one in the file. As you'll see, just one record is already pretty long. In the next phase, you'll feed these records in a pipeline that will transform these into a form that can be consumed by our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF-emneAXVaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460df2a8-9649-4b75-a5eb-e5b07438989b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "  -OEChem-12192413473D\n",
            "\n",
            " 31 30  0     1  0  0  0  0  0999 V2000\n",
            "    0.3387    0.9262    0.4600 O   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    3.4786   -1.7069   -0.3119 O   0  5  0  0  0  0  0  0  0  0  0  0\n",
            "    1.8428   -1.4073    1.2523 O   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    0.4166    2.5213   -1.2091 O   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.2359   -0.7251    0.0270 N   0  3  0  0  0  0  0  0  0  0  0  0\n",
            "   -0.7783   -1.1579    0.0914 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    0.1368   -0.0961   -0.5161 C   0  0  2  0  0  0  0  0  0  0  0  0\n",
            "   -3.1119   -1.7972    0.6590 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.4103    0.5837    0.7840 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.6433   -0.5289   -1.4260 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    1.4879   -0.6438   -0.9795 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    2.3478   -1.3163    0.1002 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    0.4627    2.1935   -0.0312 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    0.6678    3.1549    1.1001 C   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -0.7073   -2.1051   -0.4563 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -0.5669   -1.3392    1.1503 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -0.3089    0.3239   -1.4193 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.9705   -2.7295    0.1044 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.8083   -1.9210    1.7028 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -4.1563   -1.4762    0.6031 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.0398    1.4170    0.1863 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -3.4837    0.7378    0.9384 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -1.9129    0.5071    1.7551 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.2450    0.4089   -1.8190 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -2.3000   -1.3879   -2.0100 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -3.7365   -0.4723   -1.4630 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    1.3299   -1.3744   -1.7823 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    2.0900    0.1756   -1.3923 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   -0.1953    3.1280    1.7699 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    0.7681    4.1684    0.7012 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "    1.5832    2.9010    1.6404 H   0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  1  7  1  0  0  0  0\n",
            "  1 13  1  0  0  0  0\n",
            "  2 12  1  0  0  0  0\n",
            "  3 12  2  0  0  0  0\n",
            "  4 13  2  0  0  0  0\n",
            "  5  6  1  0  0  0  0\n",
            "  5  8  1  0  0  0  0\n",
            "  5  9  1  0  0  0  0\n",
            "  5 10  1  0  0  0  0\n",
            "  6  7  1  0  0  0  0\n",
            "  6 15  1  0  0  0  0\n",
            "  6 16  1  0  0  0  0\n",
            "  7 11  1  0  0  0  0\n",
            "  7 17  1  0  0  0  0\n",
            "  8 18  1  0  0  0  0\n",
            "  8 19  1  0  0  0  0\n",
            "  8 20  1  0  0  0  0\n",
            "  9 21  1  0  0  0  0\n",
            "  9 22  1  0  0  0  0\n",
            "  9 23  1  0  0  0  0\n",
            " 10 24  1  0  0  0  0\n",
            " 10 25  1  0  0  0  0\n",
            " 10 26  1  0  0  0  0\n",
            " 11 12  1  0  0  0  0\n",
            " 11 27  1  0  0  0  0\n",
            " 11 28  1  0  0  0  0\n",
            " 13 14  1  0  0  0  0\n",
            " 14 29  1  0  0  0  0\n",
            " 14 30  1  0  0  0  0\n",
            " 14 31  1  0  0  0  0\n",
            "M  CHG  2   2  -1   5   1\n",
            "M  END\n",
            "> <PUBCHEM_COMPOUND_CID>\n",
            "1\n",
            "\n",
            "> <PUBCHEM_CONFORMER_RMSD>\n",
            "0.6\n",
            "\n",
            "> <PUBCHEM_CONFORMER_DIVERSEORDER>\n",
            "2\n",
            "43\n",
            "65\n",
            "46\n",
            "25\n",
            "35\n",
            "57\n",
            "19\n",
            "53\n",
            "42\n",
            "34\n",
            "37\n",
            "41\n",
            "50\n",
            "30\n",
            "14\n",
            "13\n",
            "10\n",
            "56\n",
            "28\n",
            "55\n",
            "22\n",
            "17\n",
            "44\n",
            "52\n",
            "48\n",
            "21\n",
            "7\n",
            "61\n",
            "16\n",
            "66\n",
            "36\n",
            "12\n",
            "32\n",
            "40\n",
            "1\n",
            "24\n",
            "29\n",
            "63\n",
            "47\n",
            "9\n",
            "39\n",
            "60\n",
            "5\n",
            "20\n",
            "31\n",
            "62\n",
            "51\n",
            "4\n",
            "59\n",
            "67\n",
            "8\n",
            "18\n",
            "11\n",
            "33\n",
            "26\n",
            "6\n",
            "27\n",
            "64\n",
            "15\n",
            "58\n",
            "54\n",
            "23\n",
            "38\n",
            "3\n",
            "45\n",
            "49\n",
            "\n",
            "> <PUBCHEM_MMFF94_PARTIAL_CHARGES>\n",
            "14\n",
            "1 -0.43\n",
            "10 0.5\n",
            "11 -0.11\n",
            "12 0.91\n",
            "13 0.66\n",
            "14 0.06\n",
            "2 -0.9\n",
            "3 -0.9\n",
            "4 -0.57\n",
            "5 -1.01\n",
            "6 0.5\n",
            "7 0.28\n",
            "8 0.5\n",
            "9 0.5\n",
            "\n",
            "> <PUBCHEM_EFFECTIVE_ROTOR_COUNT>\n",
            "6\n",
            "\n",
            "> <PUBCHEM_PHARMACOPHORE_FEATURES>\n",
            "5\n",
            "1 2 acceptor\n",
            "1 3 acceptor\n",
            "1 4 acceptor\n",
            "1 5 cation\n",
            "3 2 3 12 anion\n",
            "\n",
            "> <PUBCHEM_HEAVY_ATOM_COUNT>\n",
            "14\n",
            "\n",
            "> <PUBCHEM_ATOM_DEF_STEREO_COUNT>\n",
            "0\n",
            "\n",
            "> <PUBCHEM_ATOM_UDEF_STEREO_COUNT>\n",
            "1\n",
            "\n",
            "> <PUBCHEM_BOND_DEF_STEREO_COUNT>\n",
            "0\n",
            "\n",
            "> <PUBCHEM_BOND_UDEF_STEREO_COUNT>\n",
            "0\n",
            "\n",
            "> <PUBCHEM_ISOTOPIC_ATOM_COUNT>\n",
            "0\n",
            "\n",
            "> <PUBCHEM_COMPONENT_COUNT>\n",
            "1\n",
            "\n",
            "> <PUBCHEM_CACTVS_TAUTO_COUNT>\n",
            "1\n",
            "\n",
            "> <PUBCHEM_CONFORMER_ID>\n",
            "0000000100000002\n",
            "\n",
            "> <PUBCHEM_MMFF94_ENERGY>\n",
            "37.801\n",
            "\n",
            "> <PUBCHEM_FEATURE_SELFOVERLAP>\n",
            "25.427\n",
            "\n",
            "> <PUBCHEM_SHAPE_FINGERPRINT>\n",
            "1 1 17907859857256425260\n",
            "13132413 78 18339935856441330356\n",
            "16945 1 18127404777055172104\n",
            "17841504 4 18338806718360982307\n",
            "18410436 195 18412821378365737484\n",
            "20361792 2 18413103948606886951\n",
            "20645477 70 18193836175106948431\n",
            "20653091 64 18337681930618404851\n",
            "20711985 327 18273495675867710310\n",
            "20711985 344 18052533275153547866\n",
            "21041028 32 18342473533857807689\n",
            "21061003 4 18410298003707379195\n",
            "21524375 3 17335906067529293413\n",
            "22112679 90 18128282041358100696\n",
            "23419403 2 17977062926062270852\n",
            "23552423 10 18193564595396549919\n",
            "23557571 272 18127697028774774262\n",
            "23598294 1 17832149325056171186\n",
            "2748010 2 18339911658547624660\n",
            "305870 269 17981602981145137625\n",
            "31174 14 18192722361058170003\n",
            "528862 383 18124596637411617035\n",
            "7364860 26 18197783412505576099\n",
            "81228 2 18051694343465326048\n",
            "81539 233 17831573545929999781\n",
            "\n",
            "> <PUBCHEM_SHAPE_MULTIPOLES>\n",
            "259.66\n",
            "4.28\n",
            "3.04\n",
            "1.21\n",
            "1.75\n",
            "2.55\n",
            "0.16\n",
            "-3.13\n",
            "-0.22\n",
            "-2.18\n",
            "-0.56\n",
            "0.21\n",
            "0.17\n",
            "0.09\n",
            "\n",
            "> <PUBCHEM_SHAPE_SELFOVERLAP>\n",
            "494.342\n",
            "\n",
            "> <PUBCHEM_SHAPE_VOLUME>\n",
            "160.7\n",
            "\n",
            "> <PUBCHEM_COORDINATE_TYPE>\n",
            "2\n",
            "5\n",
            "10\n",
            "\n",
            "$$$$\n"
          ]
        }
      ],
      "source": [
        "# Print one record\n",
        "!sed '/$$$$/q' {WORK_DIR}/data/00000001_00025000.sdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78FeMpgf7AJl"
      },
      "source": [
        "## Phase 2: Preprocessing\n",
        "\n",
        "The next script: `preprocess.py` uses an Apache Beam pipeline to preprocess the data. The pipeline performs the following preprocessing actions:\n",
        "\n",
        "1. Reads and parses the extracted SDF files.\n",
        "2. Counts the number of different atoms in each of the molecules in the files.\n",
        "3. Normalizes the counts to values between 0 and 1 using tf.Transform.\n",
        "4. Partitions the dataset into a training dataset and an evaluation dataset.\n",
        "5. Writes the two datasets as TFRecord objects.\n",
        "\n",
        "Apache Beam transforms can efficiently manipulate single elements at a time, but transforms that require a full pass of the dataset cannot easily be done with only Apache Beam and are better done using [tf.Transform](https://www.tensorflow.org/tfx/guide/tft). Because of this, the code uses Apache Beam transforms to read and format the molecules, and to count the atoms in each molecule. The code then uses `tf.Transform` to find the global minimum and maximum counts in order to normalize the data.\n",
        "\n",
        "The following image shows the steps in the pipeline.\n",
        "\n",
        "<img src='https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/images/etl.png' alt='https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/images/etl.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1LsKmyAAokl"
      },
      "source": [
        "### Run the preprocessing pipeline\n",
        "\n",
        "You will run the script first and the following sections will discuss the relevant parts of this code. This will take around 6 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQQ6vK2TAar8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f76d124-7fbd-4a91-b7cd-8d82dbe2c7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 05:47:06.803157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-08 05:47:07.072317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:07.072396: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-04-08 05:47:08.465480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:08.465674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:08.465699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "usage: preprocess.py [-h] --work-dir WORK_DIR\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help           show this help message and exit\n",
            "  --work-dir WORK_DIR  Directory for staging and working files. This can be a Google Cloud Storage\n",
            "                       path. (default: None)\n"
          ]
        }
      ],
      "source": [
        "# Print help documentation\n",
        "!python ./molecules/preprocess.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0D2hb4vq3Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d011bdda-fe72-447c-a33f-110b19560c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 05:47:17.136224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-08 05:47:17.471891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:17.472079: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-04-08 05:47:18.767305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:18.767487: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:18.767512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2025-04-08 05:47:20.019124: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 05:47:20.019179: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2025-04-08 05:47:20.019217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (680b1f6a7f1a): /proc/driver/nvidia/version does not exist\n",
            "2025-04-08 05:47:20.019624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
          ]
        }
      ],
      "source": [
        "# Run the preprocessing script\n",
        "!python ./molecules/preprocess.py --work-dir={WORK_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXpjPZWZzJkU"
      },
      "source": [
        "You should now have a few more outputs in your work directory. Most important are:\n",
        "\n",
        "* **transform_fn** - the transformation graph describing how to transform SDF inputs to tensors\n",
        "* **transformed_metadata** - contains the metadata describing the data types of the `tf.Transform` outputs\n",
        "* **train-dataset** - contains the transformed training dataset\n",
        "* **eval-dataset** - contains the transformed evaluation dataset\n",
        "* **PreprocessData** - pickled file containing variables you will use in the training phase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyABwtyAXpyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5619de30-1388-4b0d-c5e0-5140838cd1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  eval-dataset  PreprocessData  tft-temp  train-dataset  transformed_metadata  transform_fn\n"
          ]
        }
      ],
      "source": [
        "# List working directory\n",
        "!ls {WORK_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRU07-iR1Byo"
      },
      "source": [
        "The training and evaluation datasets contain TFRecords and you can view them by running the helper function in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sGHYX6m70LX"
      },
      "outputs": [],
      "source": [
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "# Define a helper function to get individual examples\n",
        "def get_records(dataset, num_records):\n",
        "    '''Extracts records from the given dataset.\n",
        "    Args:\n",
        "        dataset (TFRecordDataset): dataset saved in the preprocessing step\n",
        "        num_records (int): number of records to preview\n",
        "    '''\n",
        "\n",
        "    # initialize an empty list\n",
        "    records = []\n",
        "\n",
        "    # Use the `take()` method to specify how many records to get\n",
        "    for tfrecord in dataset.take(num_records):\n",
        "\n",
        "        # Get the numpy property of the tensor\n",
        "        serialized_example = tfrecord.numpy()\n",
        "\n",
        "        # Initialize a `tf.train.Example()` to read the serialized data\n",
        "        example = tf.train.Example()\n",
        "\n",
        "        # Read the example data (output is a protocol buffer message)\n",
        "        example.ParseFromString(serialized_example)\n",
        "\n",
        "        # convert the protocol bufffer message to a Python dictionary\n",
        "        example_dict = (MessageToDict(example))\n",
        "\n",
        "        # append to the records list\n",
        "        records.append(example_dict)\n",
        "\n",
        "    return records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR1F6XQY8GOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bd970d-a12b-4fd2-cb65-62e05188e3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'features': {'feature': {'Energy': {'floatList': {'value': [37.801]}},\n",
            "                           'NormalizedC': {'floatList': {'value': [0.21428572]}},\n",
            "                           'NormalizedH': {'floatList': {'value': [0.265625]}},\n",
            "                           'NormalizedN': {'floatList': {'value': [0.083333336]}},\n",
            "                           'NormalizedO': {'floatList': {'value': [0.1904762]}}}}},\n",
            " {'features': {'feature': {'Energy': {'floatList': {'value': [44.1107]}},\n",
            "                           'NormalizedC': {'floatList': {'value': [0.21428572]}},\n",
            "                           'NormalizedH': {'floatList': {'value': [0.28125]}},\n",
            "                           'NormalizedN': {'floatList': {'value': [0.083333336]}},\n",
            "                           'NormalizedO': {'floatList': {'value': [0.1904762]}}}}}]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from pprint import pprint\n",
        "\n",
        "# Create TF Dataset from TFRecord of training set\n",
        "train_data = tf.data.TFRecordDataset(f'{WORK_DIR}/train-dataset/part-00000-of-00001')\n",
        "\n",
        "# Print two records\n",
        "test_data = get_records(train_data, 2)\n",
        "\n",
        "pprint(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-76euq1v8eA"
      },
      "source": [
        "*Note: From the output cell above, you might concur that we'll need more than the atom counts to make better predictions. You'll notice that the counts are identical in both records but the `Energy` value is different. Thus, you cannot expect the model to have a low loss during the training phase later. For simplicity, we'll just use atom counts in this exercise but feel free to revise later to have more predictive features. You can share your findings in our Discourse community to discuss with other learners who are interested in the same problem.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuSdU2unjfuk"
      },
      "source": [
        "The `PreprocessData` contains Python objects needed in the training phase such as:\n",
        "\n",
        "* the filename patterns of the training and eval set directories\n",
        "* spec file describing the input features\n",
        "* name of the label column\n",
        "\n",
        "These are saved in a serialized file using [dill](https://pypi.org/project/dill/) when you ran the `preprocess` script earlier and you can deserialize it using the cell below to view its contents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix from ChatGPT\n",
        "!python3.7 -m pip install dill tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCwCMLy4BDGd",
        "outputId": "16482b69-487d-4db2-ebd2-bc99414d6cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (0.3.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.62.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.38.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.6.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix from ChatGPT\n",
        "!python3.7 -c \"import tensorflow as tf; print(tf.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjOu7nuHBXXv",
        "outputId": "ccf3e01a-d0ac-4276-a39d-fed06ab4d0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 06:09:10.603675: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:09:12.143806: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:09:12.143947: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:09:12.143976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWAQW4zDtmA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785827d8-2e99-4ea3-920c-8985c9e8d3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STDOUT:\n",
            " {'eval_files_pattern': 'results/eval-dataset/part*',\n",
            " 'input_feature_spec': {'Energy': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
            "                        'TotalC': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
            "                        'TotalH': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
            "                        'TotalN': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
            "                        'TotalO': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)},\n",
            " 'labels': ['Energy'],\n",
            " 'train_files_pattern': 'results/train-dataset/part*'}\n",
            "\n",
            "STDERR:\n",
            " 2025-04-08 06:13:42.854697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:13:44.321030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:13:44.321307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:13:44.321380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import path to the Python3.7 packages so it can find the dill module\n",
        "import sys\n",
        "sys.path.insert(0, r\"/usr/local/lib/python3.7/dist-packages\")\n",
        "\n",
        "# fix from ChatGPT\n",
        "import subprocess\n",
        "\n",
        "code = \"\"\"\n",
        "import tensorflow as tf\n",
        "import dill as pickle\n",
        "from pprint import pprint\n",
        "\n",
        "# Helper function to load the serialized file\n",
        "def load(filename):\n",
        "  with tf.io.gfile.GFile(filename, 'rb') as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "# Load PreprocessData\n",
        "preprocess_data = load('/content/results/PreprocessData')\n",
        "\n",
        "# Print contents\n",
        "pprint(vars(preprocess_data))\n",
        "\"\"\"\n",
        "\n",
        "# Run the code in a Python 3.7 subprocess and capture output\n",
        "result = subprocess.run(['python3.7', '-c', code], capture_output=True, text=True)\n",
        "\n",
        "print(\"STDOUT:\\n\", result.stdout)\n",
        "print(\"STDERR:\\n\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9p8fq4J1em6"
      },
      "source": [
        "The next sections will describe how these are implemented as a Beam pipeline in `preprocess.py`. You can open this file in a separate text editor so you can look at it more closely while reviewing the snippets below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyMkp9iD7hN4"
      },
      "source": [
        "### Applying element-based transforms\n",
        "\n",
        "The `preprocess.py` code creates an Apache Beam pipeline.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "# Build and run a Beam Pipeline\n",
        "with beam.Pipeline(options=beam_options) as p, \\\n",
        "     beam_impl.Context(temp_dir=tft_temp_dir):\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "\n",
        "Next, the code applies a `feature_extraction` transform to the pipeline.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "# Transform and validate the input data matches the input schema\n",
        "dataset = (\n",
        "    p\n",
        "    | 'Feature extraction' >> feature_extraction\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "The pipeline uses `SimpleFeatureExtraction` as its `feature_extraction` transform.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "pubchem.SimpleFeatureExtraction(pubchem.ParseSDF(data_files_pattern)),\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "The `SimpleFeatureExtraction` transform, defined in `pubchem/pipeline.py`, contains a series of transforms that manipulate all elements independently. First, the code parses the molecules from the source file, then formats the molecules to a dictionary of molecule properties, and finally, counts the atoms in the molecule. These counts are the features (inputs) for the machine learning model.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "class SimpleFeatureExtraction(beam.PTransform):\n",
        "  \"\"\"The feature extraction (element-wise transformations).\n",
        "\n",
        "  We create a `PTransform` class. This `PTransform` is a bundle of\n",
        "  transformations that can be applied to any other pipeline as a step.\n",
        "\n",
        "  We'll extract all the raw features here. Due to the nature of `PTransform`s,\n",
        "  we can only do element-wise transformations here. Anything that requires a\n",
        "  full-pass of the data (such as feature scaling) has to be done with\n",
        "  tf.Transform.\n",
        "  \"\"\"\n",
        "  def __init__(self, source):\n",
        "    super(SimpleFeatureExtraction, self).__init__()\n",
        "    self.source = source\n",
        "\n",
        "  def expand(self, p):\n",
        "    # Return the preprocessing pipeline. In this case we're reading the PubChem\n",
        "    # files, but the source could be any Apache Beam source.\n",
        "    return (p\n",
        "        | 'Read raw molecules' >> self.source\n",
        "        | 'Format molecule' >> beam.ParDo(FormatMolecule())\n",
        "        | 'Count atoms' >> beam.ParDo(CountAtoms())\n",
        "    )\n",
        "```\n",
        "</details>\n",
        "</br>\n",
        "\n",
        "The read transform `beam.io.Read(pubchem.ParseSDF(data_files_pattern))` reads SDF files from a custom source.\n",
        "\n",
        "The custom source, called `ParseSDF`, is defined in `pubchem/pipeline.py`. ParseSDF extends `FileBasedSource` and implements the `read_records` function that opens the extracted SDF files.\n",
        "\n",
        "The pipeline groups the raw data into sections of relevant information needed for the next steps. Each section in the parsed SDF file is stored in a dictionary (see `pipeline/sdf.py`), where the keys are the section names and the values are the raw line contents of the corresponding section.\n",
        "\n",
        "The code applies `beam.ParDo(FormatMolecule())` to the pipeline. The `ParDo` applies the `DoFn` named `FormatMolecule` to each molecule. `FormatMolecule` yields a dictionary of formatted molecules. The following snippet is an example of an element in the output `PCollection`:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see a sample output of <i>beam.ParDo(FormatMolecule())</i> </summary>\n",
        "\n",
        "```\n",
        "{\n",
        "  'atoms': [\n",
        "    {\n",
        "      'atom_atom_mapping_number': 0,\n",
        "      'atom_stereo_parity': 0,\n",
        "      'atom_symbol': u'O',\n",
        "      'charge': 0,\n",
        "      'exact_change_flag': 0,\n",
        "      'h0_designator': 0,\n",
        "      'hydrogen_count': 0,\n",
        "      'inversion_retention': 0,\n",
        "      'mass_difference': 0,\n",
        "      'stereo_care_box': 0,\n",
        "      'valence': 0,\n",
        "      'x': -0.0782,\n",
        "      'y': -1.5651,\n",
        "      'z': 1.3894,\n",
        "    },\n",
        "    ...\n",
        "  ],\n",
        "  'bonds': [\n",
        "    {\n",
        "      'bond_stereo': 0,\n",
        "      'bond_topology': 0,\n",
        "      'bond_type': 1,\n",
        "      'first_atom_number': 1,\n",
        "      'reacting_center_status': 0,\n",
        "      'second_atom_number': 5,\n",
        "    },\n",
        "    ...\n",
        "  ],\n",
        "  '<PUBCHEM_COMPOUND_CID>': ['3\\n'],\n",
        "  ...\n",
        "  '<PUBCHEM_MMFF94_ENERGY>': ['19.4085\\n'],\n",
        "  ...\n",
        "}\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "\n",
        "Then, the code applies `beam.ParDo(CountAtoms())` to the pipeline. The `DoFn` `CountAtoms` sums the number of carbon, hydrogen, nitrogen, and oxygen atoms each molecule has. `CountAtoms` outputs a `PCollection` of features and labels. Here is an example of an element in the output `PCollection`:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see a sample output of <i>beam.ParDo(CountAtoms())</i></summary>\n",
        "\n",
        "```\n",
        "{\n",
        "  'ID': 3,\n",
        "  'TotalC': 7,\n",
        "  'TotalH': 8,\n",
        "  'TotalO': 4,\n",
        "  'TotalN': 0,\n",
        "  'Energy': 19.4085,\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "The pipeline then validates the inputs. The `ValidateInputData` `DoFn` validates that every element matches the metadata given in the `input_schema`. This validation ensures that the data is in the correct format when it's fed into TensorFlow.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "| 'Validate inputs' >> beam.ParDo(ValidateInputData(\n",
        "    input_feature_spec)))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX0Iu8RI_wg0"
      },
      "source": [
        "### Applying full-pass transforms\n",
        "\n",
        "The Molecules code sample uses a [Deep Neural Network Regressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) to make predictions. The general recommendation is to normalize the inputs before feeding them into the ML model. The pipeline uses `tf.Transform` to normalize the counts of each atom to values between 0 and 1. To read more about normalizing inputs, see [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling).\n",
        "\n",
        "Normalizing the values requires a full pass through the dataset, recording the minimum and maximum values. The code uses `tf.Transform` to go through the entire dataset and apply full-pass transforms.\n",
        "\n",
        "To use `tf.Transform`, the code must provide a function that contains the logic of the transform to perform on the dataset. In `preprocess.py`, the code uses the `AnalyzeAndTransformDataset` transform provided by `tf.Transform`. Learn more about [how to use tf.Transform](https://www.tensorflow.org/tfx/transform/get_started).\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "# Apply the tf.Transform preprocessing_fn\n",
        "input_metadata = dataset_metadata.DatasetMetadata(\n",
        "    dataset_schema.from_feature_spec(input_feature_spec))\n",
        "\n",
        "dataset_and_metadata, transform_fn = (\n",
        "    (dataset, input_metadata)\n",
        "    | 'Feature scaling' >> beam_impl.AnalyzeAndTransformDataset(\n",
        "        feature_scaling))\n",
        "dataset, metadata = dataset_and_metadata\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "In `preprocess.py`, the `feature_scaling` function used is `normalize_inputs`, which is defined in `pubchem/pipeline.py`. The function uses the `tf.Transform` function `scale_to_0_1` to normalize the counts to values between 0 and 1.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "def normalize_inputs(inputs):\n",
        "  \"\"\"Preprocessing function for tf.Transform (full-pass transformations).\n",
        "\n",
        "  Here we will do any preprocessing that requires a full-pass of the dataset.\n",
        "  It takes as inputs the preprocessed data from the `PTransform` we specify, in\n",
        "  this case `SimpleFeatureExtraction`.\n",
        "\n",
        "  Common operations might be scaling values to 0-1, getting the minimum or\n",
        "  maximum value of a certain field, creating a vocabulary for a string field.\n",
        "\n",
        "  There are two main types of transformations supported by tf.Transform, for\n",
        "  more information, check the following modules:\n",
        "    - analyzers: tensorflow_transform.analyzers.py\n",
        "    - mappers:   tensorflow_transform.mappers.py\n",
        "\n",
        "  Any transformation done in tf.Transform will be embedded into the TensorFlow\n",
        "  model itself.\n",
        "  \"\"\"\n",
        "  return {\n",
        "      # Scale the input features for normalization\n",
        "      'NormalizedC': tft.scale_to_0_1(inputs['TotalC']),\n",
        "      'NormalizedH': tft.scale_to_0_1(inputs['TotalH']),\n",
        "      'NormalizedO': tft.scale_to_0_1(inputs['TotalO']),\n",
        "      'NormalizedN': tft.scale_to_0_1(inputs['TotalN']),\n",
        "\n",
        "      # Do not scale the label since we want the absolute number for prediction\n",
        "      'Energy': inputs['Energy'],\n",
        "  }\n",
        "```\n",
        "</details>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjoqsAGATfF"
      },
      "source": [
        "### Partitioning the dataset\n",
        "\n",
        "Next, the `preprocess.py` pipeline partitions the single dataset into two datasets. It allocates approximately 80% of the data to be used as training data, and approximately 20% of the data to be used as evaluation data.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "# Split the dataset into a training set and an evaluation set\n",
        "assert 0 < eval_percent < 100, 'eval_percent must in the range (0-100)'\n",
        "train_dataset, eval_dataset = (\n",
        "    dataset\n",
        "    | 'Split dataset' >> beam.Partition(\n",
        "        lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2))\n",
        "```\n",
        "</details>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n3oo7fxAa2w"
      },
      "source": [
        "### Writing the output\n",
        "\n",
        "Finally, the `preprocess.py` pipeline writes the two datasets (training and evaluation) using the `WriteToTFRecord` transform.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "# Write the datasets as TFRecords\n",
        "coder = example_proto_coder.ExampleProtoCoder(metadata.schema)\n",
        "\n",
        "train_dataset_prefix = os.path.join(train_dataset_dir, 'part')\n",
        "_ = (\n",
        "    train_dataset\n",
        "    | 'Write train dataset' >> tfrecordio.WriteToTFRecord(\n",
        "        train_dataset_prefix, coder))\n",
        "\n",
        "eval_dataset_prefix = os.path.join(eval_dataset_dir, 'part')\n",
        "_ = (\n",
        "    eval_dataset\n",
        "    | 'Write eval dataset' >> tfrecordio.WriteToTFRecord(\n",
        "        eval_dataset_prefix, coder))\n",
        "\n",
        "# Write the transform_fn\n",
        "_ = (\n",
        "    transform_fn\n",
        "    | 'Write transformFn' >> transform_fn_io.WriteTransformFn(work_dir))\n",
        "```\n",
        "</details>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhI7KSqLAsjM"
      },
      "source": [
        "## Phase 3: Training\n",
        "\n",
        "Recall that at the end of the preprocessing phase, the code split the data into two datasets (training and evaluation).\n",
        "\n",
        "The script uses a simple dense neural network for the regression problem. The `trainer/task.py` file contains the code for training the model. The main function of `trainer/task.py` loads the parameters needed from the preprocessing phase and passes it to the task runner function (i.e. `run_fn`).\n",
        "\n",
        "In this exercise, we will not focus too much on the training metrics (e.g. accuracy). That is discussed in other courses of this specialization. The main objective is to look at the outputs and how it is connected to the prediction phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uULHUNPa3LhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3282e53-b7b3-4860-d103-9c50c66beaf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 06:14:55.314704: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:14:57.070436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:14:57.070604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:14:57.070631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "usage: task.py [-h] --work-dir WORK_DIR\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help           show this help message and exit\n",
            "  --work-dir WORK_DIR  Directory for staging and working files. This can be a Google Cloud Storage\n",
            "                       path. (default: None)\n"
          ]
        }
      ],
      "source": [
        "# Print help documentation\n",
        "!python ./molecules/trainer/task.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOEhQ5szvd-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e775e238-18bb-43b2-c237-d4e4a8e5ba71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 06:15:03.626830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:15:04.818847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:15:04.819040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:15:04.819070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2025-04-08 06:15:06.370679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:15:06.376495: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " NormalizedC (InputLayer)       [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " NormalizedH (InputLayer)       [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " NormalizedN (InputLayer)       [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " NormalizedO (InputLayer)       [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4)            0           ['NormalizedC[0][0]',            \n",
            "                                                                  'NormalizedH[0][0]',            \n",
            "                                                                  'NormalizedN[0][0]',            \n",
            "                                                                  'NormalizedO[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          640         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            65          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,961\n",
            "Trainable params: 8,961\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "100/100 [==============================] - 4s 23ms/step - loss: 690.9979 - accuracy: 4.6875e-04 - val_loss: 314.3922 - val_accuracy: 0.0033\n",
            "WARNING:absl:Function `serve_tf_examples_fn` contains input name(s) ID, TotalC, TotalH, TotalN, TotalO with unsupported characters which will be renamed to id, totalc, totalh, totaln, totalo in the SavedModel.\n"
          ]
        }
      ],
      "source": [
        "# Run the trainer.\n",
        "!python ./molecules/trainer/task.py --work-dir {WORK_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm7x_Jx-3wdi"
      },
      "source": [
        "The outputs of this phase are in the `model` directory. This will be the trained model that you will use for predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umUBl5byXxOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94a2cea-c26d-49b7-a33d-9bfab714e840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\tfingerprint.pb\tkeras_metadata.pb  saved_model.pb  variables\n"
          ]
        }
      ],
      "source": [
        "!ls {WORK_DIR}/model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9wB5HNK5pdt"
      },
      "source": [
        "The important thing to note in the training script is it also exports the transformation graph with the model. That is shown in these lines:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "# Define default serving signature\n",
        "signatures = {\n",
        "    'serving_default':\n",
        "        _get_serve_tf_examples_fn(model,\n",
        "                                  tf_transform_output, input_feature_spec).get_concrete_function(\n",
        "                                      [signatures_dict])\n",
        "}\n",
        "\n",
        "# Save model with signature\n",
        "model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures, include_optimizer=False)\n",
        "```\n",
        "</details>\n",
        "\n",
        "The implementation of `_get_serve_tf_examples_fn()` is as follows:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output, feature_spec):\n",
        "  \"\"\"Returns a function that applies data transformation and generates predictions\"\"\"\n",
        "\n",
        "  # Get transformation graph\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function\n",
        "  def serve_tf_examples_fn(inputs_list):\n",
        "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "    \n",
        "    # Create a shallow copy of the dictionary in the single element list\n",
        "    inputs = inputs_list[0].copy()\n",
        "\n",
        "    # Pop ID since it is not needed in the transformation graph\n",
        "    # Also needed to identify predictions\n",
        "    id_key = inputs.pop('ID')\n",
        "    \n",
        "    # Apply data transformation to the raw inputs\n",
        "    transformed = model.tft_layer(inputs)\n",
        "\n",
        "    # Pass the transformed data to the model to get predictions\n",
        "    predictions = model(transformed.values())\n",
        "\n",
        "    return id_key, predictions\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "```\n",
        "</details>\n",
        "\n",
        "The use of `model.tft_layer` means that your model can accept raw data and it will do the transformation before feeding it to make predictions. It implies that when you serve your model for predictions, you don't have to worry about creating a pipeline to transform new data coming in. The model will already do that for you through this serving input function. It helps to prevent training-serving skew since you're handling the training and serving data the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIL-vfymBx-S"
      },
      "source": [
        "## Phase 4: Prediction\n",
        "\n",
        "After training the model, you can provide the model with inputs and it will make predictions. The pipeline in `predict.py` is responsible for making predictions. It reads the input files from the custom source and writes the output predictions as text files to the specified working directory.\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "if args.verb == 'batch':\n",
        "  data_files_pattern = os.path.join(args.inputs_dir, '*.sdf')\n",
        "  results_prefix = os.path.join(args.outputs_dir, 'part')\n",
        "  source = pubchem.ParseSDF(data_files_pattern)\n",
        "  sink = beam.io.WriteToText(results_prefix)\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "\n",
        "The following image shows the steps in the prediction pipeline:\n",
        "\n",
        "<img src='https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/images/predict.png' alt='https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course4/week2-ungraded-labs/C4_W2_Lab_4_ETL_Beam/images/predict.png'>\n",
        "\n",
        "In `predict.py`, the code defines the pipeline in the run function:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "def run(model_dir, feature_extraction, sink, beam_options=None):\n",
        "  print('Listening...')\n",
        "  with beam.Pipeline(options=beam_options) as p:\n",
        "    _ = (p\n",
        "        | 'Feature extraction' >> feature_extraction\n",
        "        | 'Predict' >> beam.ParDo(Predict(model_dir, 'ID'))\n",
        "        | 'Format as JSON' >> beam.Map(json.dumps)\n",
        "        | 'Write predictions' >> sink)\n",
        "```\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "The code calls the run function with the following parameters:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "run(\n",
        "    args.model_dir,\n",
        "    pubchem.SimpleFeatureExtraction(source),\n",
        "    sink,\n",
        "    beam_options)\n",
        "```\n",
        "</details>\n",
        "\n",
        "First, the code passes the `pubchem.SimpleFeatureExtraction(source)` transform as the `feature_extraction` transform. This transform, which was also used in the preprocessing phase, is applied to the pipeline:\n",
        "\n",
        "<details>\n",
        "<summary> Click here to see the code snippet </summary>\n",
        "\n",
        "```\n",
        "class SimpleFeatureExtraction(beam.PTransform):\n",
        "  \"\"\"The feature extraction (element-wise transformations).\n",
        "\n",
        "  We create a `PTransform` class. This `PTransform` is a bundle of\n",
        "  transformations that can be applied to any other pipeline as a step.\n",
        "\n",
        "  We'll extract all the raw features here. Due to the nature of `PTransform`s,\n",
        "  we can only do element-wise transformations here. Anything that requires a\n",
        "  full-pass of the data (such as feature scaling) has to be done with\n",
        "  tf.Transform.\n",
        "  \"\"\"\n",
        "  def __init__(self, source):\n",
        "    super(SimpleFeatureExtraction, self).__init__()\n",
        "    self.source = source\n",
        "\n",
        "  def expand(self, p):\n",
        "    # Return the preprocessing pipeline. In this case we're reading the PubChem\n",
        "    # files, but the source could be any Apache Beam source.\n",
        "    return (p\n",
        "        | 'Read raw molecules' >> self.source\n",
        "        | 'Format molecule' >> beam.ParDo(FormatMolecule())\n",
        "        | 'Count atoms' >> beam.ParDo(CountAtoms())\n",
        "    )\n",
        "```\n",
        "</details>\n",
        "\n",
        "The transform reads from the appropriate source based on the pipeline’s execution mode (i.e. batch), formats the molecules, and counts the different atoms in each molecule.\n",
        "\n",
        "Next, `beam.ParDo(Predict(…))` is applied to the pipeline that performs the prediction of the molecular energy. `Predict`, the `DoFn` that's passed, uses the given dictionary of input features (atom counts), to predict the molecular energy.\n",
        "\n",
        "The next transform applied to the pipeline is `beam.Map(lambda result: json.dumps(result))`, which takes the prediction result dictionary and serializes it into a JSON string. Finally, the output is written to the sink.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WzZvOIBPWJe"
      },
      "source": [
        "### Batch predictions\n",
        "\n",
        "Batch predictions are optimized for throughput rather than latency. Batch predictions work best if you're making many predictions and you can wait for all of them to finish before getting the results. You can run the following cells to use the script to run batch predictions. For simplicity, you will use the same file you used for training. If you want however, you can use the data extractor script earlier to grab a different SDF file and feed it here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raPGxPCK6VwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86b56ed-fa38-4b5f-e984-884d968cda22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 06:16:52.369594: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:16:53.842053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:16:53.842267: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:16:53.842299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "usage: predict.py [-h] --work-dir WORK_DIR --model-dir MODEL_DIR {batch,stream} ...\n",
            "\n",
            "positional arguments:\n",
            "  {batch,stream}\n",
            "    batch               Batch prediction\n",
            "    stream              Streaming prediction\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --work-dir WORK_DIR   Directory for temporary files and preprocessed datasets to. This can be a\n",
            "                        Google Cloud Storage path. (default: None)\n",
            "  --model-dir MODEL_DIR\n",
            "                        Path to the exported TensorFlow model. This can be a Google Cloud Storage\n",
            "                        path. (default: None)\n"
          ]
        }
      ],
      "source": [
        "# Print help documentation. You can ignore references to GCP and streaming data.\n",
        "!python ./molecules/predict.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXTi7NPJ6My1"
      },
      "outputs": [],
      "source": [
        "# Define model, input and output data directories\n",
        "MODEL_DIR = f'{WORK_DIR}/model'\n",
        "DATA_DIR = f'{WORK_DIR}/data'\n",
        "PRED_DIR = f'{WORK_DIR}/predictions'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTc-Q9q7sa65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b73324d-9514-435c-b926-9a43e1b4a675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 06:17:09.182582: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:17:10.427745: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:17:10.427959: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:17:10.427985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Listening...\n",
            "2025-04-08 06:17:12.110567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-04-08 06:17:12.110620: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        }
      ],
      "source": [
        "# Run batch prediction. This can take up to 7 minutes.\n",
        "!python ./molecules/predict.py \\\n",
        "  --model-dir {MODEL_DIR} \\\n",
        "  --work-dir {WORK_DIR} \\\n",
        "  batch \\\n",
        "  --inputs-dir {DATA_DIR} \\\n",
        "  --outputs-dir {PRED_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFhSEvY99z_7"
      },
      "source": [
        "The results should now be in the `predictions` folder. This is just a text file so you can easily print the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6P6vtPmX5w1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1e4449-28a2-44e4-f78e-3fe691d44b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\t      model\t   PreprocessData  train-dataset\t transform_fn\n",
            "eval-dataset  predictions  tft-temp\t   transformed_metadata\n"
          ]
        }
      ],
      "source": [
        "# List working directory\n",
        "!ls {WORK_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1-Mq0JO6WPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a9ed2f-3ca6-4c36-ca2b-30aabd8fd8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": [1], \"prediction\": [12.703414916992188]}\n",
            "{\"id\": [2], \"prediction\": [12.310404777526855]}\n",
            "{\"id\": [3], \"prediction\": [0.38077810406684875]}\n",
            "{\"id\": [4], \"prediction\": [5.428731918334961]}\n",
            "{\"id\": [5], \"prediction\": [13.357125282287598]}\n",
            "{\"id\": [6], \"prediction\": [30.092571258544922]}\n",
            "{\"id\": [7], \"prediction\": [63.805599212646484]}\n",
            "{\"id\": [8], \"prediction\": [-1.985792636871338]}\n",
            "{\"id\": [9], \"prediction\": [7.040426254272461]}\n",
            "{\"id\": [11], \"prediction\": [-9.55524730682373]}\n",
            "{\"id\": [12], \"prediction\": [0.37226006388664246]}\n",
            "{\"id\": [13], \"prediction\": [-5.984096527099609]}\n",
            "{\"id\": [14], \"prediction\": [3.161376476287842]}\n",
            "{\"id\": [16], \"prediction\": [23.624927520751953]}\n",
            "{\"id\": [17], \"prediction\": [15.044429779052734]}\n",
            "{\"id\": [18], \"prediction\": [5.2377800941467285]}\n",
            "{\"id\": [19], \"prediction\": [1.1667959690093994]}\n",
            "{\"id\": [20], \"prediction\": [1.1838324069976807]}\n",
            "{\"id\": [21], \"prediction\": [-1.1997759342193604]}\n",
            "{\"id\": [22], \"prediction\": [-1.2082936763763428]}\n",
            "{\"id\": [23], \"prediction\": [16.92827606201172]}\n",
            "{\"id\": [26], \"prediction\": [74.02787017822266]}\n",
            "{\"id\": [28], \"prediction\": [14.249896049499512]}\n",
            "{\"id\": [29], \"prediction\": [10.768460273742676]}\n",
            "{\"id\": [30], \"prediction\": [12.366046905517578]}\n",
            "{\"id\": [31], \"prediction\": [2.4781992435455322]}\n",
            "{\"id\": [32], \"prediction\": [4.934487819671631]}\n",
            "{\"id\": [33], \"prediction\": [-7.278392791748047]}\n",
            "{\"id\": [34], \"prediction\": [-8.064412117004395]}\n",
            "{\"id\": [35], \"prediction\": [0.7652692198753357]}\n",
            "{\"id\": [36], \"prediction\": [0.6925883889198303]}\n",
            "{\"id\": [37], \"prediction\": [2.56791615486145]}\n",
            "{\"id\": [38], \"prediction\": [-1.1997759342193604]}\n",
            "{\"id\": [39], \"prediction\": [-2.297603130340576]}\n",
            "{\"id\": [40], \"prediction\": [-0.10194729268550873]}\n",
            "{\"id\": [41], \"prediction\": [2.1749074459075928]}\n",
            "{\"id\": [42], \"prediction\": [4.951523780822754]}\n",
            "{\"id\": [43], \"prediction\": [0.6755517721176147]}\n",
            "{\"id\": [44], \"prediction\": [1.444533348083496]}\n",
            "{\"id\": [45], \"prediction\": [0.6585156321525574]}\n",
            "{\"id\": [46], \"prediction\": [7.826444625854492]}\n",
            "{\"id\": [47], \"prediction\": [-3.0836212635040283]}\n",
            "{\"id\": [48], \"prediction\": [13.45535659790039]}\n",
            "{\"id\": [49], \"prediction\": [-3.092139482498169]}\n",
            "{\"id\": [50], \"prediction\": [4.451761245727539]}\n",
            "{\"id\": [51], \"prediction\": [1.4615693092346191]}\n",
            "{\"id\": [58], \"prediction\": [-3.100656747817993]}\n",
            "{\"id\": [59], \"prediction\": [3.247180700302124]}\n",
            "{\"id\": [61], \"prediction\": [8.505707740783691]}\n",
            "{\"id\": [62], \"prediction\": [-4.886267185211182]}\n",
            "{\"id\": [63], \"prediction\": [21.993267059326172]}\n",
            "{\"id\": [64], \"prediction\": [-3.39543080329895]}\n",
            "{\"id\": [65], \"prediction\": [-2.6094141006469727]}\n",
            "{\"id\": [66], \"prediction\": [-4.493258953094482]}\n",
            "{\"id\": [67], \"prediction\": [5.2377800941467285]}\n",
            "{\"id\": [68], \"prediction\": [6.638900279998779]}\n",
            "{\"id\": [69], \"prediction\": [0.38077810406684875]}\n",
            "{\"id\": [70], \"prediction\": [-3.0836212635040283]}\n",
            "{\"id\": [71], \"prediction\": [1.470088243484497]}\n",
            "{\"id\": [72], \"prediction\": [1.1667959690093994]}\n",
            "{\"id\": [73], \"prediction\": [82.66960144042969]}\n",
            "{\"id\": [75], \"prediction\": [6.214751243591309]}\n",
            "{\"id\": [77], \"prediction\": [0.6925883889198303]}\n",
            "{\"id\": [78], \"prediction\": [8.49160385131836]}\n",
            "{\"id\": [79], \"prediction\": [22.164186477661133]}\n",
            "{\"id\": [80], \"prediction\": [2.2646238803863525]}\n",
            "{\"id\": [81], \"prediction\": [9.7188081741333]}\n",
            "{\"id\": [82], \"prediction\": [12.102413177490234]}\n",
            "{\"id\": [83], \"prediction\": [0.2655068635940552]}\n",
            "{\"id\": [85], \"prediction\": [9.623505592346191]}\n",
            "{\"id\": [86], \"prediction\": [13.1605863571167]}\n",
            "{\"id\": [87], \"prediction\": [-3.886674404144287]}\n",
            "{\"id\": [89], \"prediction\": [29.733638763427734]}\n",
            "{\"id\": [91], \"prediction\": [-0.7000135183334351]}\n",
            "{\"id\": [92], \"prediction\": [31.079044342041016]}\n",
            "{\"id\": [93], \"prediction\": [1.470088243484497]}\n",
            "{\"id\": [95], \"prediction\": [4.451761245727539]}\n",
            "{\"id\": [96], \"prediction\": [-3.100656747817993]}\n",
            "{\"id\": [98], \"prediction\": [-3.1091747283935547]}\n",
            "{\"id\": [101], \"prediction\": [-2.600895881652832]}\n",
            "{\"id\": [102], \"prediction\": [-3.3869130611419678]}\n",
            "{\"id\": [104], \"prediction\": [0.37226006388664246]}\n",
            "{\"id\": [105], \"prediction\": [4.033198833465576]}\n",
            "{\"id\": [106], \"prediction\": [15.240970611572266]}\n",
            "{\"id\": [107], \"prediction\": [-2.583859920501709]}\n",
            "{\"id\": [108], \"prediction\": [11.276739120483398]}\n",
            "{\"id\": [109], \"prediction\": [11.86628532409668]}\n",
            "{\"id\": [110], \"prediction\": [0.6585156321525574]}\n",
            "{\"id\": [111], \"prediction\": [24.654613494873047]}\n",
            "{\"id\": [112], \"prediction\": [6.318570613861084]}\n",
            "{\"id\": [113], \"prediction\": [5.92556095123291]}\n",
            "{\"id\": [114], \"prediction\": [11.293774604797363]}\n",
            "{\"id\": [115], \"prediction\": [17.141857147216797]}\n",
            "{\"id\": [116], \"prediction\": [17.739849090576172]}\n",
            "{\"id\": [117], \"prediction\": [38.22043991088867]}\n",
            "{\"id\": [118], \"prediction\": [6.223268032073975]}\n",
            "{\"id\": [119], \"prediction\": [8.1071138381958]}\n",
            "{\"id\": [120], \"prediction\": [1.158277988433838]}\n",
            "{\"id\": [122], \"prediction\": [4.943004608154297]}\n",
            "{\"id\": [123], \"prediction\": [48.65070343017578]}\n"
          ]
        }
      ],
      "source": [
        "# Print the first 100 results\n",
        "!head -n 100 /content/results/predictions/part-00000-of-00001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCdFIa_I-Rea"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "You've now completed all phases of the Beam-based pipeline! Similar processes are done under the hood by higher-level frameworks such as TFX and you can use the techniques here to understand their codebase better or to extend them for your own needs. As mentioned earlier, the [original article](https://cloud.google.com/dataflow/examples/molecules-walkthrough) also offers the option to use GCP and to perform online predictions as well. Feel free to try it out but be aware of the recurring costs.\n",
        "\n",
        "On to the next part of the course!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
